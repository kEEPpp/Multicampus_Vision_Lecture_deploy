{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Public AI</b></i>\n",
    "<br>\n",
    "###  &nbsp;&nbsp; **✎&nbsp;&nbsp;Week 1. DNN Basis**\n",
    "# Section 6. 신경망 학습의 핵심, 역전파\n",
    "\n",
    "\n",
    "### _Objective_\n",
    "1. **주요 미분 공식**: 신경망의 핵심인 역전파를 이해하는 데 필수적인 미분공식을 배웁니다. <br>\n",
    "2. **Backropagation(역전파)**: 신경망에서 역전파란 무엇인지, 그리고 왜 필요한지 알아봅니다.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 1. 주요 미분 공식 \\]\n",
    "\n",
    "역전파 알고리즘에서는 미분에 대한 이해가 필수적입니다. 이를 위해, 주요 미분 공식에 대해 알아보는 시간을 갖도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 도함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 도함수의 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $y=f(x)$를 미분한다는 의미는 아래와 같이 계산하는 것입니다.\n",
    "\n",
    "$$f\\prime (x)=\\lim _{ \\Delta x\\rightarrow 0 } \\frac { { f(x+\\Delta x)-f(x) } }{ \\Delta x }$$\n",
    "\n",
    "그리고 이 $f\\prime(x)$를 $f(x)$의 도함수 또는 미분이라고합니다.<br>\n",
    "표기할 때는 $f\\prime(x)$ 또는 $\\frac { dy }{ dx } , \\frac { d }{ dx }f(x) $와 같이 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 도함수와 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "흔히 미분에 대해 생각할 때 가장 먼저 떠오르는 것은 `접선의 기울기`일 것입니다. <br>\n",
    "왜 $x=a$인 지점에서 함수 $y=f(x)$의 접선의 기울기가 $f\\prime(a)$일까요? 이는 위의 도함수 식을 통해 알 수 있습니다. <br>\n",
    "\n",
    "$$f\\prime (a)=\\lim _{ \\Delta x\\rightarrow 0 } \\frac { { f(a+\\Delta x)-f(a) } }{ \\Delta x }$$\n",
    "\n",
    "위의 식에서 $\\Delta x$가 0으로 근사하지 않고 0보다 충분히 큰 경우 이것은 접선의 기울기가 아니고 두 점 $(a, f(a)),~(a+\\Delta x, f(a+\\Delta x))$ 사이에서 변화된 정도를 나타냅니다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/YvKCMsP.png\" width=\"450\" height=\"345\"/>\n",
    "\n",
    "$$ \\frac { f(a+\\Delta x) - f(a) }{ (a+\\Delta x) - a } = \\frac { f(a+\\Delta x) - f(a) }{ \\Delta x} $$ <br>\n",
    "\n",
    "이때 $ \\Delta x\\rightarrow 0 $ 과 같이 극한을 취하면 $ (a+\\Delta x, f(a+\\Delta x)) $가 $ (a, f(a)) $에 가까워 지고 결국 $ f\\prime (a) $가 접선의 기울기와 일치하게 됩니다. <br>\n",
    "\n",
    "![](https://i.imgur.com/py4OBwa.png)\n",
    "\n",
    "이렇게 구한 접선의 기울기 $ f\\prime (a) $를 $ x=a $지점에서 $ y=f(x) $의 미분계수라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 여러가지 미분법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 다항식 미분\n",
    "\n",
    "|$f(x)$   |$f'(x)$|\n",
    "|------|-------|\n",
    "|$c$|$0$|\n",
    "|$ax+b$|$a$|\n",
    "|$x^n$|$nx^{n-1}$|\n",
    "|$kx^n$|$nkx^{n-1}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 1\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = 2x^3+3x^2+5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 2\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{2}{x^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 지수 로그 미분\n",
    "\n",
    "* 딥러닝에서 로그(log)는 기본적으로 자연로그(ln)로 취급합니다.\n",
    "\n",
    "|$$f(x)$$|$$f'(x)$$|\n",
    "|------|-------|\n",
    "|$$e^x$$|$$e^x$$|\n",
    "|$$a^x$$|$$a^xlog(a)$$|\n",
    "|$$logx$$|$$\\frac{1}{x}$$|\n",
    "|$$log_ax$$|$$\\frac{1}{x}\\frac{1}{log a}$$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 1\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = 2e^{3x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 2\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = 3log_5x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 주요 미분 법칙\n",
    "\n",
    "1.  합의 미분<br>\n",
    "$\n",
    "y = f(x)+g(x) \\rightarrow y' = f'(x) + g'(x)\n",
    "$\n",
    "\n",
    "2. 곱의 미분<br>\n",
    "$\n",
    "y = f(x)g(x) \\rightarrow y' = f'(x)g(x) + g'(x)f(x)\n",
    "$\n",
    "\n",
    "3. 합성함수 미분<br>\n",
    "$\n",
    "y = f(g(x)) \\rightarrow y' = f'(g(x))g'(x)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 1\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = e^{3x} + log(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 2\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = xlog(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 3\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = e^{3x^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 편미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편미분이란 다변수함수를 미분하는 것을 말합니다.<br>\n",
    "다시 말해 두 개 이상의 변수를 포함한 함수에서 그 중 하나의 변수에 관해서만 미분하는 것을 말합니다.<br>\n",
    "간단하게 생각하면 하나의 변수에 관해서만 미분하므로 다른 변수들은 상수로 간주하고 미분을 진행하는 것입니다.<br>\n",
    "\n",
    "위에서 정의했던 미분식과 같이 $z = f(x,y)$의 편미분을 다음과 같이 표현할 수 있습니다.<br>\n",
    "\n",
    "$$\\frac {\\partial z} {\\partial x}=\\lim _{ \\Delta x\\rightarrow 0 } \\frac { { f(x+\\Delta x,y)-f(x,y) } }{ \\Delta x }$$\n",
    "\n",
    "$$\\frac {\\partial z} {\\partial x}=\\lim _{ \\Delta y\\rightarrow 0 } \\frac { { f(x+\\Delta x,y)-f(x,y) } }{ \\Delta y }$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 주요 미분 법칙\n",
    "1. 합의 미분<br>\n",
    "$\\frac { \\partial }{ \\partial x }((f(x,y) + g(x,y))~=~ \\frac { \\partial }{ \\partial x }f(x,y) + \\frac { \\partial }{ \\partial x }g(x,y)$\n",
    "2. 곱의 미분<br>\n",
    "$\\frac { \\partial }{ \\partial x }((f(x,y)g(x,y))~=~ (\\frac { \\partial }{ \\partial x }f(x,y))g(x,y)+f(x,y)(\\frac { \\partial }{ \\partial x }g(x,y))$\n",
    "3. 나눗셈의 미분<br>\n",
    "$\\frac { \\partial }{ \\partial x }(\\frac{f(x,y)}{g(x,y)})~=~ \\frac{(\\frac { \\partial }{ \\partial x }f(x,y))g(x,y)-f(x,y)(\\frac { \\partial }{ \\partial x }g(x,y))}{(g(x,y))^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 1\n",
    "\n",
    "> $\\frac{\\partial z}{\\partial x}$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "z = 3x^4+x^3-3y^3-2x^2y+6xy^2-5y+1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 2\n",
    "\n",
    "> $\\frac{\\partial z}{\\partial x}$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "z = \\frac{(2x^4+3y^2)(3x^3-y^2)}{3x^2y^3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 연쇄 법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파를 더 쉽게 이해하기 위해 위에서 배웠던 합성함수의 미분 공식에 대해 자세히 알아보겠습니다. 함수 $y=f(u)$와 $u=g(x)$가 있고 이를 합치면 $y=f(g(x))$가 됩니다. 이를 **합성함수**라 합니다. <br>\n",
    "합성함수의 도함수는 위에서 배웠듯이 아래와 같습니다. <br>\n",
    "\n",
    "$$\n",
    "\\frac { dy }{ dx  } =  \\frac { dy }{ du }  \\cdot \\frac { du }{ dx } = f\\prime (g(x)) \\cdot g\\prime(x) \n",
    "$$\n",
    "\n",
    "이를 간단한 예시를 들어 설명하겠습니다.\n",
    "\n",
    "$$\n",
    "y=\\log(\\frac{1}{x})\n",
    "$$\n",
    "\n",
    "이 식을 어떻게 미분할 수 있을까요? 먼저 아래와 같이 함수를 두개로 분리합니다.\n",
    "\n",
    "$$\n",
    "y=\\log u \\\\\n",
    "~u=\\frac{1}{x}\n",
    "$$\n",
    "\n",
    "그리고 각각을 미분합니다.\n",
    "\n",
    "$$\n",
    "\\frac{dy}{du} = \\frac{1}{u} \\\\\n",
    "\\frac{du}{dx} = -\\frac{1}{x^2}\n",
    "$$\n",
    "\n",
    "그리고 합성함수의 미분공식을 적용합니다.\n",
    "\n",
    "$$\n",
    "\\frac { dy }{ dx  } =  \\frac { dy }{ du }  \\cdot \\frac { du }{ dx } = \\frac{1}{u} \\cdot (-\\frac{1}{x^2}) = -\\frac{1}{x}\n",
    "$$\n",
    "\n",
    "이처럼 합성함수의 도함수가 각 도함수 끼리의 곱의 형태로 이루어지는 것을 **연쇄법칙(chain rule)**이라고 합니다. <br>\n",
    "이는 편미분에서도 동일하게 적용됩니다. <br>\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} * \\frac{\\partial t}{\\partial x} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 1\n",
    "\n",
    "> $\\frac{\\partial z}{\\partial x}$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "z = (x+y)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 2. 역전파 개괄 \\]\n",
    "\n",
    "심층 신경망은 기본적으로 입력 $x$를 받아서 출력 $\\hat y$를 산출합니다. 정보는 신경망을 따라 앞으로 흘러가는 이 과정을 이전까지 배웠듯이, **FeedForward(순전파)**라고 합니다. 앞서 순전파를 배우면서 가중치 값을 임의로 생성하여 만든 후, 예측값을 구했었습니다. 이렇게 얻은 예측값은 정확한 값일까요? 그렇지 않을 것입니다.\n",
    "\n",
    "로지스틱 회귀 모델에서 최선의 y 값을 예측할 수 있는 모델을 만들기 위해, y의 실제 값과 예측값의 오차를 줄이는 방향으로 최선의 가중치($w$)를 찾아 모델을 훈련하는 과정을 거친다고 배웠었습니다. 신경망에서도 이와 마찬가지의 과정이 필요하며, 그 모델 훈련 과정을 \"역전파\"라고 부릅니다. 역전파 과정에서는 초기에 설정한 가중치 값을 통과하여 순전파를 통해 얻어진 결과값으로 손실 정보를 계산하고, 신경망을 따라 다시 거꾸로(역방향으로) 흘러가며 기울기를 계산하여 오차(손실 정보)를 줄이는 방향으로 가중치를 갱신합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 신경망의 학습 알고리즘\n",
    "\n",
    "\n",
    "기본적으로 신경망의 학습은 선형 회귀, 로지스틱 회귀와 마찬가지로 Gradient Descent 알고리즘을 이용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Gradient Descent 알고리즘 복습\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/ks7q74o.png\" width=\"800\" height=\"600\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gradient descent 수식**\n",
    "\n",
    "$\n",
    "W_{i+1} = W_i - \\alpha * \\frac{\\partial L}{\\partial W}\n",
    "$\n",
    "\n",
    "Graident Descent 방법으로 최선의 가중치(Weight, $W$) 구하기 위해서는 각 **Weight 별 기울기 $(\\frac{\\partial L}{\\partial W})$** 를 구하고, Loss $L$을 최소화하는 방향으로 가중치를 개선하는 과정을 반복합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 심층 신경망에서의 Gradient Descent\n",
    "\n",
    "그렇다면 심층 신경망에서는 Gradient Descent가 어떻게 적용될까요?\n",
    "\n",
    "예제로 아래와 같은 3층 신경망이 있다고 생각해봅시다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/qRH21Ws.png\" width=\"800\" height=\"600\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent 알고리즘으로 위와 같은 모델에 있는 가중치 값을 찾고자 합니다. 각 층의 가중치는 아래와 같은 수식으로 갱신됩니다.<br>\n",
    "\n",
    "* 첫번째 은닉층의 weight, bias update\n",
    "$$\n",
    "W^{[1]}_{new} = W^{[1]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial W^{[1]}}\\\\\n",
    "b^{[1]}_{new} = b^{[1]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial b^{[1]}}\n",
    "$$\n",
    "\n",
    "* 두번째 은닉층의 weight, bias update\n",
    "$$\n",
    "W^{[2]}_{new} = W^{[2]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial W^{[2]}}\\\\\n",
    "b^{[2]}_{new} = b^{[2]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial b^{[2]}}\n",
    "$$\n",
    "\n",
    "* 출력층의 weight, bias update\n",
    "$$\n",
    "W^{[3]}_{new} = W^{[3]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial W^{[3]}}\\\\\n",
    "b^{[3]}_{new} = b^{[3]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial b^{[3]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Gradient Descent 알고리즘을 적용해 갱신된 가중치($W_{new}$)를 얻기 위해서, 총 6 종류의 Gradient\n",
    "($\n",
    "\\frac{\\partial Loss}{\\partial W^{[1]}},\n",
    "\\frac{\\partial Loss}{\\partial b^{[1]}},\n",
    "\\frac{\\partial Loss}{\\partial W^{[2]}},\n",
    "\\frac{\\partial Loss}{\\partial b^{[2]}},\n",
    "\\frac{\\partial Loss}{\\partial W^{[3]}},\n",
    "\\frac{\\partial Loss}{\\partial b^{[3]}}\n",
    "$)을 계산해야 합니다.\n",
    "\n",
    "어떻게 구할 수 있을까요? 이때 활용되는 것이 바로 역전파(BackPropagation) 알고리즘입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BackPropagation의 필요성\n",
    "\n",
    "역전파(BackPropagation) 알고리즘은 심층 신경망 내 가중치들의 Gradient를 구하는 방법입니다. 역전파는 오차 수열의 점화식으로 구성되어 있습니다. 역전파 없이 Gradient를 구하는 경우와 역전파를 이용해 Gradient를 구하는 경우를 비교해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) BackPropagation 없이 계산하기\n",
    "\n",
    "<img src=\"https://i.imgur.com/tlDSSKI.png\" width=\"800\" height=\"600\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^{[1]}_{1,1}$에 대한 Weights Gradient $\\frac{dL}{dW^{[1]}_{1,1}}$을 계산하기 위해서는 아래와 같이 장황한 Gradient 식이 필요합니다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/qGUKEDY.png\" width=\"800\" height=\"600\"/><br>\n",
    "\n",
    "깊이가 깊어질수록, Gradient 수식은 훨씬 더 장황해집니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) BackPropagation의 아이디어 : 점화식 패턴 이용하기\n",
    "\n",
    "오차 역전파법은 알고리즘 중 **Dynamic Programming**의 한 패턴입니다. 이미 계산된 부분을 저장하고, 이를 반복적으로 재적용해가는 식으로 이용됩니다.<br>\n",
    "\n",
    "아래 이미지에서 초록색으로 표시한 부분은 이전에 계산했던 부분입니다. 다시 계산하는 것이 아닌 이전에 계산한 부분을 재적용하면서, Gradient를 재귀적으로 구해가는 과정을 **오차역전파법(BackPropagation)**이라고 부릅니다.<br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/CcAScJy.png\" width=\"800\" height=\"600\"/><br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/WQ5FnP4.png\" width=\"800\" height=\"600\"/><br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/l4tt4y3.png\" width=\"800\" height=\"600\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "---\n",
    "\n",
    "    Copyright(c) 2019 by Public AI. All rights reserved.\n",
    "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/09/16\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
