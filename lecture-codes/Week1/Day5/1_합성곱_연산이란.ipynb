{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Public AI***\n",
    "###  &nbsp;&nbsp; **✎&nbsp;&nbsp;Week 9. CNN Basis**\n",
    "# Section 1. 합성곱 연산이란\n",
    "\n",
    "### _Objective_\n",
    "1. 기존의 DNN 방식에서 FullyConnectedLayer(이하 FC)  을 통해 이미지 처리를 하는 법을 배워 봅니다\n",
    "\n",
    "2. 이미지 처리에 딥러닝을 적용시에 어떤 문제점이 발생되는지 파악해 봅니다. \n",
    "\n",
    "2. 이미지의 기본적인 특성을 배워 보고 이미지의 특성을 활용해 딥러닝을 적용하는 법에 대해 배워 봅니다. \n",
    "\n",
    "3. 대부분의 합성곱 연산은 4차원의 데이터를 다룹니다. 이를 어떻게 연산하는 지를 배워봅니다. <br>\n",
    "\n",
    "4. 합성곱 연산의 연산량을 줄여주는 풀링 연산에 대해 배워봅니다.<br> \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 1. FC Layer 을 활용해 이미지 처리 하기 \\]\n",
    "\n",
    "----\n",
    "\n",
    "----\n",
    "\n",
    "> 이미지를 한줄로 Flatten 한 후 기존 방법 처럼 FC Layer 을 연결합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/publicai/opt/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# import needed library \n",
    "import tensorflow as tf \n",
    "import tensorflow.python.keras as keras \n",
    "from tensorflow.python.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tf.get_default_graph()\n",
    "\n",
    "# fashion mnist 가져오기\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = train_images/ 255.\n",
    "test_images = test_images/ 255.\n",
    "\n",
    "# model \n",
    "inputs = Input(shape=(28,28))\n",
    "\n",
    "# ** 이렇게 이미지를 한줄로 편 후 Deep Learning Model 에 연결합니다.** \n",
    "flat_inputs = Flatten()(inputs)\n",
    "\n",
    "dense1 = Dense(128, activation='relu')(flat_inputs)\n",
    "dense2 = Dense(128, activation='relu')(dense1)\n",
    "pred = Dense(10, activation='softmax')(dense2)\n",
    "\n",
    "model = Model(inputs, pred)\n",
    "# optimizer \n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)\n",
    "model.compile('sgd', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 3s 47us/sample - loss: 0.9330 - acc: 0.7036 - val_loss: 0.6214 - val_acc: 0.7952\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5795 - acc: 0.8044 - val_loss: 0.5294 - val_acc: 0.8198\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.5150 - acc: 0.8236 - val_loss: 0.4822 - val_acc: 0.8297\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.4803 - acc: 0.8341 - val_loss: 0.4638 - val_acc: 0.8363\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.4587 - acc: 0.8400 - val_loss: 0.4445 - val_acc: 0.8412\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.4419 - acc: 0.8459 - val_loss: 0.4569 - val_acc: 0.8390\n",
      "Epoch 7/10\n",
      "31680/54000 [================>.............] - ETA: 0s - loss: 0.4260 - acc: 0.8520"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ca84e7504f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m hist = model.fit(train_images, train_labels,\n\u001b[0;32m----> 2\u001b[0;31m                  batch_size=64, validation_split=0.1, epochs=10)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_images, train_labels,\n",
    "                 batch_size=64, validation_split=0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image을 FC Layer 에 연결시 단점 \n",
    "![Imgur](https://i.imgur.com/r5SO4i2.png)\n",
    "\n",
    "- 이미지를 구별하기 위해 필요하지 않는 정보들을 weights 에 연결해야 해 weights 갯수가 많이 필요하다는 단점이 있습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 2. 동물이 사물의 형태를 인지하는 방법 \\]\n",
    "\n",
    "- 동물들은 간단한 저차원 (점, 선) 특징들을 추출한 후 추출된 특징들을 조합해 점차 복잡한 이미지를 구성합니다. \n",
    "\n",
    "[Receptive fields of single neurones in the cat's striate cortex\n",
    "D. H. Hubel and T. N. Wiesel](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/)\n",
    "\n",
    "![Imgur](https://i.imgur.com/rf3kDoy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 3. 이미지의 특징 \\]\n",
    "![Imgur](https://i.imgur.com/FgoyoPW.png)\n",
    "- 이미지는 픽셀과 인접해 있는 픽셀간의 값의 차이를 통해 이미지의 특징을 표현할수 있습니다.\n",
    "- 국소적인 특징들을 조합해 복잡한 특징들을 나타낼수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 4. 이미지의 구조 \\]\n",
    "\n",
    "> 이미지가 어떻게 컴퓨터에서 보여지고 구조화 되어 있는지 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**컴퓨터 비전**은 컴퓨터를 이용하여 정지영상 또는 동영상으로부터 의미있는 정보를 추출하는 방법을 연구하는 학문입니다. 이미지를 분석하기 위해선 그에 대한 정보가 필요하기 때문입니다. 컴퓨터 비전을 간단하게 정리하면 사람이 눈으로 보고 인지하는 작업을 컴퓨터가 동등하게 수행할 수 있게끔 연구하는 학문이라고 할 수 있습니다. 이런 작업이 우리에게 왜 필요할까요?\n",
    "\n",
    "컴퓨터 비전은 이미지로부터 의미있는 정보를 추출하는 것을 다룹니다. 사람이 사물을 보고 무엇인지 인지하고 그에 대한 정보를 추출하는 것은 매우 쉬운 작업이지만, 컴퓨터가 사물을 인식할 수 있도록 하는 일은 매우 어렵습니다. 이를 예시를 통해 살펴보겠습니다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/0UOiXGp.png\" width=\"800\" height=\"600\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리 인간은 눈으로 사과와 토마토를 각각 인식할 수 있습니다. 그러나 컴퓨터의 경우는 인간의 경우와 전혀 다릅니다. <br>\n",
    "첫번째 그림과 같은 사과 사진을 컴퓨터의 input으로 주고, 이 사진의 객체를 사과라고 인식하는 문제에 대해 생각해 보겠습니다. 흰 배경을 제외한 가운데 영역에 빨간색 성분이 많이 있고 둥근 윤곽을 가지면 사과라고 인식하게끔 프로그램을 만들면 빨간 사과를 인식할 수 있을 것입니다. 그러나 두번째 그림처럼 초록색 사과도 함께 인식해야 한다면 사과의 색상정보에 초록색도 추가해야 합니다. 만약 세번째 그림처럼 빨간색 토마토가 입력으로 들어오면 상황은 더 복잡해집니다. 색상정보와 윤곽선 정보만으로는 사과와 토마토가 구분이 되지 않으므로 꼭지의 모양까지 고려해야 합니다. 네번째 그림처럼 배경이 단순하지 않고 여러 과일이 겹쳐 있는 경우에는 인식이 더더욱 어려워지겠죠. \n",
    "\n",
    "이러한 문제들을 컴퓨터 비전에서는 영상으로부터 유용한 정보를 추출하고, 이를 조합하여 결과를 유추합니다. 컴퓨터 비전에서 주로 사용하는 영상정보는 밝기, 색상, 모양 등이 있으며, 이러한 정보와 머신러닝, 딥러닝 알고리즘을 함께 사용하여 사물을 인지할 수 있습니다. 그러나 영상으로부터 이러한 정보들을 추출하는 것은 쉬운일이 아닙니다. 예를들어 배경과 객체를 어떻게 구분해야 하는지, 빨간색을 판단하기 위해서는 어떤 수식을 사용해야 하는지, 둥근 윤곽을 판단하기 위해서는 어떤 알고리즘을 사용해야 하는지 등을 결정하는 것은 쉬운 문제가 아닙니다. 앞으로 수업시간에는 영상으로부터 유용한 정보를 추출하는 방법과 추출된 정보를 효과적으로 사용하는 방법들에 대해 다루게 될것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Image?\n",
    "\n",
    "아래 이미지는 조르주 쇠라의 '그랑트 자트 섬의 센 강, 봄' 입니다. 인상주의 이후 선보인 '점묘법'에 의해 그려진 그림으로, 자잘한 점을 캔버스 표면에 나란히 찍어가며 풍경을 묘사하고 있습니다. \n",
    "\n",
    "<img src=\"https://gscaltexmediahub.com/wp-content/uploads/2018/10/GSC_BS_MH_gs-calender-2019-04_20181026_01.jpg\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "컴퓨터가 이미지를 이해하는 **픽셀**의 아이디어가 위와 같은 점묘법에서 출발했습니다. 여기서 픽셀은 이미지를 이루는 낱낱의 점들을 말하고, 하나의 픽셀은 하나의 밝기 또는 색상을 표현하며, 이러한 픽셀이 모여서 2차원 영상을 구성합니다. 컴퓨터는 이미지를 가장 먼저 픽셀로 이해하며, 이 픽셀은 일련의 숫자로 그 값을 표시합니다. 예를들어 컴퓨터의 입장에서 1024 * 768의 해상도를 가진 이미지는 1024 * 768 = 786432개의 점, 즉 786432개의 픽셀을 포함하는 1024개의 열과 768 개의 행이 있는 **행렬형태**로 존재하게 됩니다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/goyV2hO.png\" width=\"800\" height=\"600\"/>\n",
    "\n",
    "컴퓨터 비전에서는 주로 grayscale image와 truecolor image를 사용합니다. Grayscale image는 흑백 영상과 같이 밝기 정보로만 구성된 영상을 의미하고, 회색조 영상이라고도 합니다. 반면에 다양한 색상을 표현할 수 있는 영상을 컬러 영상이라고합니다. 그레이스케일 영상은 밝기 정보를 256단계로 구분하여 표현합니다. 그레이스케일 영상에서 하나의 픽셀은 0부터 255사이의 정수 값으로 256단계를 표현합니다. 가장 밝음을 나타내는 255에서, 빛이 없음을 나타내는 0까지의 그라데이션으로 밝기를 표현합니다. 컬러영상은 기본적으로 R, G, B 세 개의 색상 성분 조합으로 픽셀 값을 표현합니다. 한 픽셀당 R채널, G채널, B채널의 각각의 배열에, 각각의 색상 성분은 0부터 255사이의 정수 값으로 표현되며 이들의 조합으로 이미지의 색을 나타냅니다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/XiWSz2n.png\" width=\"800\" height=\"600\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
